{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Bharath Narayanan Venkatesh\n",
    "#### Student ID: s4033348\n",
    "\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* os\n",
    "* warnings\n",
    "\n",
    "## Introduction\n",
    "Online e-commerce platforms are highly developing and in order to improve their recommendation process, many companies are hiring data analysts to work on their data which comprises of customer reviews and recommendations.We can build a classification model to analyse the customer reviews. For that we have to first perform the essential text pre-processing on Review Text column present in the dataset. Text Pre Processing is one of the basic steps in NLP. They are used to remove noise and irrelevant information from the target column. This Review Text column contains customer reviews of different clothes. This column provides good information about the customers mindset and their feelings about the product. Since the content present in that column is not structured, I am transforiming this raw text into a more structured format for further analysis using various tasks like Tokenization, lowercasing, removing short words and stop words, calculate term frequency and document frequency, removing words that appear only once, and finally saving them. By doing this, we are giving a new dimension to the column but at the same time we are keeping the content as it is. This makes it easier to perform Machine Learning analysis too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "I am defining the filepath in the first step. In the second step, I am trying to get the information about the file size before loading the file.Last step is the one which provides information about the structure of the file by printing out few lines. Then I am loading the file and then to know the structure, I am using .info and head to print out few lines. Clothing ID, Age, Rating, Recommended IND, Positive Feedback Count are all int type. Title, Review Text, Division Name, Department Name and Class Name are all string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 7.14 MB\n",
      "Clothing ID,Age,Title,Review Text,Rating,Recommended IND,Positive Feedback Count,Division Name,Department Name,Class Name\n",
      "\n",
      "1077,60,Some major design flaws,\"I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c\",3,0,0,General,Dresses,Dresses\n",
      "\n",
      "1049,50,My favorite buy!,\"I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\",5,1,0,General Petite,Bottoms,Pants\n",
      "\n",
      "847,47,Flattering shirt,This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!,5,1,6,General,Tops,Blouses\n",
      "\n",
      "1080,49,Not for the very petite,\"I love tracy reese dresses, but this one is not for the very petite. i am just under 5 feet tall and usually wear a 0p in this brand. this dress was very pretty out of the package but its a lot of dress. the skirt is long and very full so it overwhelmed my small frame. not a stranger to alterations, shortening and narrowing the skirt would take away from the embellishment of the garment. i love the color and the idea of the style but it just did not work on me. i returned this dress.\",2,0,4,General,Dresses,Dresses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file\n",
    "file_path = 'assignment3.csv'\n",
    "\n",
    "# Getting information about file size\n",
    "file_size = os.path.getsize(file_path)\n",
    "print(f\"File size: {file_size / (1024 * 1024):.2f} MB\")  # Convert bytes to MB\n",
    "\n",
    "# Reading the first few lines to know the structure of file\n",
    "with open(file_path, 'r') as file:\n",
    "    for i in range(5):\n",
    "        print(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19662 entries, 0 to 19661\n",
      "Data columns (total 10 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Clothing ID              19662 non-null  int64 \n",
      " 1   Age                      19662 non-null  int64 \n",
      " 2   Title                    19662 non-null  object\n",
      " 3   Review Text              19662 non-null  object\n",
      " 4   Rating                   19662 non-null  int64 \n",
      " 5   Recommended IND          19662 non-null  int64 \n",
      " 6   Positive Feedback Count  19662 non-null  int64 \n",
      " 7   Division Name            19662 non-null  object\n",
      " 8   Department Name          19662 non-null  object\n",
      " 9   Class Name               19662 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 1.5+ MB\n",
      "None\n",
      "   Clothing ID  Age                    Title  \\\n",
      "0         1077   60  Some major design flaws   \n",
      "1         1049   50         My favorite buy!   \n",
      "2          847   47         Flattering shirt   \n",
      "3         1080   49  Not for the very petite   \n",
      "4          858   39     Cagrcoal shimmer fun   \n",
      "\n",
      "                                         Review Text  Rating  Recommended IND  \\\n",
      "0  I had such high hopes for this dress and reall...       3                0   \n",
      "1  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
      "2  This shirt is very flattering to all due to th...       5                1   \n",
      "3  I love tracy reese dresses, but this one is no...       2                0   \n",
      "4  I aded this in my basket at hte last mintue to...       5                1   \n",
      "\n",
      "   Positive Feedback Count   Division Name Department Name Class Name  \n",
      "0                        0         General         Dresses    Dresses  \n",
      "1                        0  General Petite         Bottoms      Pants  \n",
      "2                        6         General            Tops    Blouses  \n",
      "3                        4         General         Dresses    Dresses  \n",
      "4                        1  General Petite            Tops      Knits  \n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Structure details\n",
    "print(df.info())  \n",
    "print(df.head())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Since we know the data type and structure of the file, we move on to the next step which is Pre-processing. The steps include\n",
    "1. Loading stopwords_en.txt, this contains large number of stopwords, which we usually remove during pre-processing and then printing the first 10 lines of that file.\n",
    "2. Tokenization process is the next step. I am using the same regular expression as mentioned in the Assignment specification.I am defining a function preprocessing_text to perform multiple tasks like Tokenization and lowercasing the review text column, removing short words that are less than 2 characters, removing stop words and finally storing them in a new column named Processed_Review.\n",
    "3. We are removing the words in Processed_Review based on term frequency and document frequency. I am removing the words that appear only 1 time and also top 20 most frequent words. To view the words I am converting filtered_vocab to a list named filtered_vocab_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a's\n",
      "able\n",
      "about\n",
      "above\n",
      "according\n",
      "accordingly\n",
      "across\n",
      "actually\n",
      "after\n"
     ]
    }
   ],
   "source": [
    "# Path to stopwords file\n",
    "stopwords = 'stopwords_en.txt'\n",
    "\n",
    "# Read and print first 10 lines of stopwords\n",
    "with open(stopwords, 'r') as file:\n",
    "    for i in range(10):\n",
    "        print(file.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization regular expression\n",
    "token_pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "\n",
    "def preprocessing_text(review):\n",
    "    #Tokenization and lowercasing\n",
    "    tokens = re.findall(token_pattern, review.lower())\n",
    "    \n",
    "    # Removing the short words that are less than 2 characters\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    # Removing stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Storing processed tokens in new column\n",
    "df['Processed_Review'] = df['Review Text'].fillna('').apply(preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joke', 'bib', 'endowed', 'vertical', 'hoodie', 'thats', 'sorts', 'wid', 'skipped', 're-ordered', 'mesmerizing', 'reed', 'belts', 'attempted', 'sleeved', 'unclear', 'lumpiness', 'willingness', 'picture', 'newly', 'stetson', 'honey', 'striped', 'maintaining', 'literally', 'characteristic', 'encompasses', 'store', 'volumous', 'limited', 'marvelous', 'intertwined', 'bitter', 'bosom', 'actually', 'fan', 'jackie', 'peite', 'capes', 'fyi', 'caused', 'options', 'creme', 'a-symmetric', 'luxuriously', 'offwhite', 'length', 'exit', 'sur', 'madras', 'intentions', 'lithe', 'intrigued', 'lumpy', 'which', 'cascades', 'push', 'horizon', 'mid-rise', 'fits', 'poolside', 'holder', 'trousers', 'sandpaper', 'treat', 'thinks', 'favs', 'growing', 'bicep', 'dresse', 'noting', 'bags', 'scene', 'knee', 'happily', 'rue', 'double-lined', 'synthetic', 'hair', 'monday', 'levis', 'exception', 'post', 'speck', 'silky', 'remover', 'feeling', 'prettiest', 'describes', 'beadwork', 'minuet', 'hold', 'nights', 'leans', 'attaches', 'representation', 'purposes', 'must-have', 'hunt', 'emphasized']\n"
     ]
    }
   ],
   "source": [
    "# Initializing counters for term frequency and document frequency\n",
    "term_freq = Counter()\n",
    "doc_freq = Counter()\n",
    "\n",
    "# Looping through  Processed review column(newly created column) to calculate term and document frequencies\n",
    "for tokens in df['Processed_Review']:\n",
    "    term_freq.update(tokens)\n",
    "    doc_freq.update(set(tokens))\n",
    "    \n",
    "#Removing words that appear only once and also I am removing top 20 most frequent words\n",
    "top_20_words = {word for word, _ in doc_freq.most_common(20)}\n",
    "filtered_vocab = {word for word, freq in term_freq.items() if freq > 1 and word not in top_20_words}\n",
    "\n",
    "# Converting filtered_vocab to list to get first 100 words\n",
    "filtered_vocab_list = list(filtered_vocab)[:100]\n",
    "print(filtered_vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "I am rejoining the tokens that are present in the filtered_vocab and then saving them to a csv named processed.csv. I am sorting the filtered_vocab first and then I am creating a file named vocab.txt to save these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejoining tokens that are in the filtered vocabulary for each review\n",
    "df['Processed_Review'] = df['Processed_Review'].apply(lambda tokens: ' '.join([token for token in tokens if token in filtered_vocab]))\n",
    "\n",
    "# Save to a CSV file\n",
    "df.to_csv('processed.csv', index=False)\n",
    "\n",
    "# Building the vocabulary by sorting first and then save it to vocab.txt\n",
    "vocab = sorted(filtered_vocab)\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for idx, word in enumerate(vocab):\n",
    "        f.write(f\"{word}:{idx}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have performed the basic Pre-Processing of the Review Text column which includes Tokenization, Lowercasing, removal of short and stop words and removed words that appear only once as well as the top 20 most frequent words. We have saved the work to a new csv named processed.csv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
